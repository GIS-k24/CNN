"""
环境：
      torch                         1.2.0+cpu
      torchvision                   0.4.0+cpu
"""

import numpy as np
import torch
from torch import nn
from torch.autograd import Variable
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader


def conv_block(in_channel, out_channel):
    layer = nn.Sequential(
        nn.BatchNorm2d(in_channel),
        nn.ReLU(),
        nn.Conv2d(in_channel, out_channel, 3, padding=1, bias=False)
    )
    return layer


class dense_block(nn.Module):
    def __init__(self, in_channel, growth_rate, num_layers):
        super(dense_block, self).__init__()
        block = []
        channel = in_channel
        for i in range(num_layers):
            block.append(conv_block(channel, growth_rate))
            channel += growth_rate

        self.net = nn.Sequential(*block)

    def forward(self, x):
        for layer in self.net:
            out = layer(x)
            x = torch.cat((out, x), dim=1)
        return x


"""
# demo测试
test_net = dense_block(3, 12, 3)
test_x = Variable(torch.zeros(1, 3, 96, 96))
print("input shape:{}*{}*{}".format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))
result_x = test_net(test_x)
print("input shape:{}*{}*{}".format(result_x.shape[1], result_x.shape[2], result_x.shape[3]))
"""


# 由于DenseNet会不断地对维度进行拼接，所以当层数很高的适合，输出的通道数会越来越大，参数和计算量会越来越大，因此引入过渡层将输出通道降下来，过渡层用1*1的卷积层

def transition(in_channel, out_channel):
    trans_layer = nn.Sequential(
        nn.BatchNorm2d(in_channel),
        nn.ReLU(),
        nn.Conv2d(in_channel, out_channel, 1),
        nn.AvgPool2d(2, 2)
    )
    return trans_layer


"""
# demo示例：验证过渡层的正确性
test_net = transition(3, 12)
test_x = Variable(torch.zeros(1, 3, 96, 96))
print("input shape:{}*{}*{}".format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))
result_x = test_net(test_x)
print("input shape:{}*{}*{}".format(result_x.shape[1], result_x.shape[2], result_x.shape[3]))
"""


class densenet(nn.Module):
    def __init__(self, in_channel, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16]):
        super(densenet, self).__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(in_channel, 64, 7, 2, 3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, 2, padding=1)
        )

        channels = 64
        block = []
        for i, layers in enumerate(block_layers):
            block.append(dense_block(channels, growth_rate, layers))
            channels += layers * growth_rate
            if i != len(block_layers) - 1:
                block.append(transition(channels, channels // 2))  # 通过transition层将大小减半，通道数减半
                channels = channels // 2

        self.block2 = nn.Sequential(*block)
        self.block2.add_module('bn', nn.BatchNorm2d(channels))
        self.block2.add_module('relu', nn.ReLU())
        self.block2.add_module('avg_pool', nn.AvgPool2d(3))

        self.classifier = nn.Linear(channels, num_classes)

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)

        x = x.view(x.shape[0], -1)
        x = self.classifier(x)
        return x


"""
# demo网络测试
test_net = densenet(3, 10)
test_x = Variable(torch.zeros(1, 3, 96, 96))
result_x = test_net(test_x)
print("input:{}".format(result_x.shape))
"""


def data_tf(x):
    x = x.resize((96, 96), 2)  # 将图片放大到96 * 96
    x = np.array(x, dtype='float32') / 255
    x = (x - 0.5) / 0.5
    x = x.transpose((2, 0, 1))
    x = torch.from_numpy(x)
    return x


train_set = CIFAR10(
    root='D:\编程\python\PyTorch\img',
    train=True,
    transform=data_tf
)
train_data = DataLoader(
    dataset=train_set,
    batch_size=64,
    shuffle=True
)
test_set = CIFAR10(
    root='D:\编程\python\PyTorch\img',
    train=False,
    transform=data_tf
)
test_data = DataLoader(
    dataset=test_set,
    batch_size=128,
    shuffle=False
)

net = densenet(3, 10)

optimizer = torch.optim.SGD(net.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

for e in range(20):
    train_loss = 0
    for im, label in train_data:
        im = Variable(im)
        label = Variable(label)

        out = net(im)
        loss = criterion(out, label)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
    print('epoch:{},Train Loss:{:.6f}'.format(e, train_loss / len(train_data)))

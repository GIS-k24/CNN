"""
环境：
      torch                         1.2.0+cpu
      torchvision                   0.4.0+cpu

第一个真正意义上的深层网络（19层） -- 大量使用小卷积

可以看到VGG中常用kernel_size=3，而AlexNet中常用kernel_size=5，原因在于kernel_size小，即使是多层的情况参数也少一些

此外，VGG的网络模型过程相同，因此使用循环建立网络
"""

import numpy as np
import torch
from torch import nn
from torch.autograd import Variable
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader


def vgg_block(num_convs, in_channels, out_channels):
    net = [nn.Conv2d(
        in_channels,
        out_channels,
        kernel_size=3,
        padding=1
    ), nn.ReLU()]  # 定义第一层

    for i in range(num_convs - 1):  # 定义多层卷积层
        net.append(nn.Conv2d(
            out_channels,
            out_channels,
            kernel_size=3,
            padding=1
        ))
        net.append(nn.ReLU())

    net.append(nn.MaxPool2d(2, 2))  # 池化层
    return nn.Sequential(*net)


block_demo = vgg_block(3, 64, 128)
# print(block_demo)

# 第一层的in_channels和图片的通道数量不等，因此需要进行预处理
"""
用demo测试
input_demo = Variable(torch.zeros(1, 64, 300, 300))
output_demo = block_demo(input_demo)
print(output_demo.shape)
"""


def vgg_stack(num_convs, channels):
    net = []
    for n, c in zip(num_convs, channels):
        in_c = c[0]
        out_c = c[1]
        net.append(vgg_block(n, in_c, out_c))
    return nn.Sequential(*net)


# 实例测试
vgg_net = vgg_stack((1, 1, 2, 2, 2), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))
"""
# print(vgg_net)

test_x = Variable(torch.zeros(1, 3, 256, 256))
test_y = vgg_net(test_x)
print(test_y.shape)
"""


class vgg(nn.Module):
    def __init__(self):
        super(vgg, self).__init__()
        self.feature = vgg_net
        self.fc = nn.Sequential(
            nn.Linear(512, 100),
            nn.ReLU(),
            nn.Linear(100, 10)
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.shape[0], -1)
        x = self.fc(x)


def data_tf(x):
    x = np.array(x, dtype='float32') / 255
    x = (x - 0.5) / 0.5
    x = x.transpose((2, 0, 1))  # 将channel放到第一维，只是pytorch要求的输入方式
    x = torch.from_numpy(x)
    return x


train_set = CIFAR10(
    root='D:\编程\python\PyTorch\img',
    train=True,
    transform=data_tf
)

train_data = DataLoader(
    train_set, batch_size=64, shuffle=True
)

test_set = CIFAR10(
    root='D:\编程\python\PyTorch\img',
    train=False,
    transform=data_tf

)
test_data = DataLoader(
    test_set, batch_size=128, shuffle=False
)

net = vgg()
optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)
criterion = nn.CrossEntropyLoss()

for e in range(20):
    train_loss = 0
    for im, label in train_data:
        im = Variable(im)
        label = Variable(label)

        out = net(im)
        loss = criterion(out, label)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
    print('epoch:{},Train Loss:{:.6f}'.format(e, train_loss / len(train_data)))
